{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMZip1OjBqmjQCjh5ZSBv2W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/INIKA-N/Neural-Network/blob/main/ANN_for_Iris_Dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVIRylwSb21o",
        "outputId": "28450da5-6d5c-4aab-c13e-38de282069fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 0.0715068917596507\n",
            "Epoch 0, Loss: 0.07382587551148101\n",
            "Epoch 0, Loss: 0.429080062644501\n",
            "Epoch 0, Loss: 0.07205268236967435\n",
            "Epoch 0, Loss: 0.06863003328363164\n",
            "Epoch 0, Loss: 0.44684519035404635\n",
            "Epoch 0, Loss: 0.40710711147259576\n",
            "Epoch 0, Loss: 0.07003269464781514\n",
            "Epoch 0, Loss: 0.07192001209713261\n",
            "Epoch 0, Loss: 0.07533373996625661\n",
            "Epoch 0, Loss: 0.4243390673711551\n",
            "Epoch 0, Loss: 0.38310544596280954\n",
            "Epoch 0, Loss: 0.40897911229156725\n",
            "Epoch 0, Loss: 0.07222804209435141\n",
            "Epoch 0, Loss: 0.07150784127056244\n",
            "Epoch 0, Loss: 0.5188444570095927\n",
            "Epoch 0, Loss: 0.4075095320014958\n",
            "Epoch 0, Loss: 0.37680765198042515\n",
            "Epoch 0, Loss: 0.43121623884064847\n",
            "Epoch 0, Loss: 0.3737877294127099\n",
            "Epoch 0, Loss: 0.439992001448238\n",
            "Epoch 0, Loss: 0.40003582339948246\n",
            "Epoch 0, Loss: 0.40716077903220715\n",
            "Epoch 0, Loss: 0.07175833155655245\n",
            "Epoch 0, Loss: 0.401445821288166\n",
            "Epoch 0, Loss: 0.4670188086180629\n",
            "Epoch 0, Loss: 0.06995035065328394\n",
            "Epoch 0, Loss: 0.07096944467797836\n",
            "Epoch 0, Loss: 0.07924779806814498\n",
            "Epoch 0, Loss: 0.5061653583893488\n",
            "Epoch 0, Loss: 0.42442526525861385\n",
            "Epoch 0, Loss: 0.07889152085367566\n",
            "Epoch 0, Loss: 0.06735467883705668\n",
            "Epoch 0, Loss: 0.066977216018607\n",
            "Epoch 0, Loss: 0.44516454386494053\n",
            "Epoch 0, Loss: 0.07147219872240738\n",
            "Epoch 0, Loss: 0.42529308829625895\n",
            "Epoch 0, Loss: 0.29970842202781534\n",
            "Epoch 0, Loss: 0.07189880131606075\n",
            "Epoch 0, Loss: 0.4293902331099455\n",
            "Epoch 0, Loss: 0.4112512575104511\n",
            "Epoch 0, Loss: 0.07610787116115432\n",
            "Epoch 0, Loss: 0.39982646255984977\n",
            "Epoch 0, Loss: 0.40247982029924345\n",
            "Epoch 0, Loss: 0.4761531694903735\n",
            "Epoch 0, Loss: 0.39491187134239103\n",
            "Epoch 0, Loss: 0.3782221789807144\n",
            "Epoch 0, Loss: 0.44904025049523877\n",
            "Epoch 0, Loss: 0.07799315834733968\n",
            "Epoch 0, Loss: 0.5178824602329244\n",
            "Epoch 0, Loss: 0.4003076667242817\n",
            "Epoch 0, Loss: 0.07248821928356534\n",
            "Epoch 0, Loss: 0.06899931851574498\n",
            "Epoch 0, Loss: 0.47266797672431377\n",
            "Epoch 0, Loss: 0.39177594123196413\n",
            "Epoch 0, Loss: 0.07274532930232307\n",
            "Epoch 0, Loss: 0.40210735423877497\n",
            "Epoch 0, Loss: 0.07528603061022737\n",
            "Epoch 0, Loss: 0.07383569916420694\n",
            "Epoch 0, Loss: 0.3729783743372974\n",
            "Epoch 0, Loss: 0.5297323237935282\n",
            "Epoch 0, Loss: 0.3952810875217247\n",
            "Epoch 0, Loss: 0.3832462699985054\n",
            "Epoch 0, Loss: 0.36774884895169585\n",
            "Epoch 0, Loss: 0.41127644984523476\n",
            "Epoch 0, Loss: 0.4345599744331176\n",
            "Epoch 0, Loss: 0.07560455928479741\n",
            "Epoch 0, Loss: 0.07795670154291366\n",
            "Epoch 0, Loss: 0.40195846972327565\n",
            "Epoch 0, Loss: 0.3433895005137279\n",
            "Epoch 0, Loss: 0.07846902487301925\n",
            "Epoch 0, Loss: 0.07570359084001611\n",
            "Epoch 0, Loss: 0.07653665616273096\n",
            "Epoch 0, Loss: 0.4009129331757744\n",
            "Epoch 0, Loss: 0.37230979782155826\n",
            "Epoch 0, Loss: 0.07838406495830556\n",
            "Epoch 0, Loss: 0.3875607724496182\n",
            "Epoch 0, Loss: 0.37819326739521897\n",
            "Epoch 0, Loss: 0.07569169257293092\n",
            "Epoch 0, Loss: 0.3750404822805702\n",
            "Epoch 0, Loss: 0.39887744827587407\n",
            "Epoch 0, Loss: 0.38873939587922535\n",
            "Epoch 0, Loss: 0.3681890845612879\n",
            "Epoch 0, Loss: 0.3849797628041121\n",
            "Epoch 0, Loss: 0.08228218237245931\n",
            "Epoch 0, Loss: 0.3851822708765309\n",
            "Epoch 0, Loss: 0.3994115666021008\n",
            "Epoch 0, Loss: 0.37710413820155153\n",
            "Epoch 0, Loss: 0.479035911932039\n",
            "Epoch 0, Loss: 0.4178724458048145\n",
            "Epoch 0, Loss: 0.36505031324317144\n",
            "Epoch 0, Loss: 0.05027227918261676\n",
            "Epoch 0, Loss: 0.37711192808057287\n",
            "Epoch 0, Loss: 0.39329021570726846\n",
            "Epoch 0, Loss: 0.08129096767901603\n",
            "Epoch 0, Loss: 0.39784333781226877\n",
            "Epoch 0, Loss: 0.34137781705915393\n",
            "Epoch 0, Loss: 0.3467362059333982\n",
            "Epoch 0, Loss: 0.08275934160169388\n",
            "Epoch 0, Loss: 0.4827904204087186\n",
            "Epoch 0, Loss: 0.3780448204140377\n",
            "Epoch 0, Loss: 0.3323991968571865\n",
            "Epoch 0, Loss: 0.08424291405339848\n",
            "Epoch 0, Loss: 0.3788048335468832\n",
            "Epoch 0, Loss: 0.07986618310952731\n",
            "Epoch 0, Loss: 0.3536417400792857\n",
            "Epoch 0, Loss: 0.35727617177142923\n",
            "Epoch 0, Loss: 0.3351303636383651\n",
            "Epoch 0, Loss: 0.3955959491528364\n",
            "Epoch 0, Loss: 0.3362409320615387\n",
            "Epoch 0, Loss: 0.3913649212523926\n",
            "Epoch 0, Loss: 0.38160071586238686\n",
            "Epoch 0, Loss: 0.3531262351328845\n",
            "Epoch 0, Loss: 0.3371329432347275\n",
            "Epoch 0, Loss: 0.08376719327032878\n",
            "Epoch 0, Loss: 0.4004311285780725\n",
            "Epoch 0, Loss: 0.35360258156342383\n",
            "Epoch 0, Loss: 0.08294382965410536\n",
            "Epoch 0, Loss: 0.4001177471468646\n",
            "Epoch 0, Loss: 0.35120500042109914\n",
            "Test Accuracy: 100.00%\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Assume we are interested in binary classification (setosa or non-setosa)\n",
        "y_binary = np.where(y == 0, 1, 0)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        self.weights_input_hidden = np.random.randn(input_size, hidden_size)\n",
        "        self.bias_hidden = np.zeros((1, hidden_size))\n",
        "        self.weights_hidden_output = np.random.randn(hidden_size, output_size)\n",
        "        self.bias_output = np.zeros((1, output_size))\n",
        "\n",
        "    def sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def sigmoid_derivative(self, x):\n",
        "        return x * (1 - x)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.hidden_input = np.dot(inputs, self.weights_input_hidden) + self.bias_hidden\n",
        "        self.hidden_output = self.sigmoid(self.hidden_input)\n",
        "        self.final_input = np.dot(self.hidden_output, self.weights_hidden_output) + self.bias_output\n",
        "        self.final_output = self.sigmoid(self.final_input)\n",
        "        return self.final_output\n",
        "\n",
        "    def backward(self, inputs, target, learning_rate):\n",
        "        error = target - self.final_output\n",
        "        output_delta = error * self.sigmoid_derivative(self.final_output)\n",
        "\n",
        "        error_hidden = output_delta.dot(self.weights_hidden_output.T)\n",
        "        hidden_delta = error_hidden * self.sigmoid_derivative(self.hidden_output)\n",
        "\n",
        "        self.weights_hidden_output += self.hidden_output.T.dot(output_delta) * learning_rate\n",
        "        self.bias_output += np.sum(output_delta, axis=0, keepdims=True) * learning_rate\n",
        "        self.weights_input_hidden += inputs.T.dot(hidden_delta) * learning_rate\n",
        "        self.bias_hidden += np.sum(hidden_delta, axis=0, keepdims=True) * learning_rate\n",
        "\n",
        "    def train(self, inputs, targets, epochs, learning_rate):\n",
        "        for epoch in range(epochs):\n",
        "            for input_data, target_data in zip(inputs, targets):\n",
        "                input_data = np.array([input_data])  # Convert input to 2D array\n",
        "                target_data = np.array([target_data])  # Convert target to 2D array\n",
        "\n",
        "                # Forward pass\n",
        "                output = self.forward(input_data)\n",
        "\n",
        "                # Backward pass\n",
        "                self.backward(input_data, target_data, learning_rate)\n",
        "\n",
        "                # Print the loss (MSE) for every 100 epochs\n",
        "                if epoch % 100 == 0:\n",
        "                    loss = np.mean(np.square(target_data - output))\n",
        "                    print(f\"Epoch {epoch}, Loss: {loss}\")\n",
        "\n",
        "# Create a neural network with input nodes equal to the number of features (4 in the case of Iris)\n",
        "nn = NeuralNetwork(input_size=X_train.shape[1], hidden_size=5, output_size=1)\n",
        "\n",
        "# Train the neural network\n",
        "nn.train(inputs=X_train, targets=y_train, epochs=9, learning_rate=0.01)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "predictions = np.round(nn.forward(X_test))\n",
        "accuracy = accuracy_score(y_test, predictions) * 100\n",
        "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
      ]
    }
  ]
}